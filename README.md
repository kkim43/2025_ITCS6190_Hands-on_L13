# 2025 ITCS6190 Hands-on L13  
## Serverless Spark ETL Pipeline on AWS

**Name (ID):** Kiyoung Kim (801426261)  
**Email:** kkim43@charlotte.edu  

This project implements a **fully automated, event-driven, serverless ETL pipeline** using:  
- AWS S3  
- AWS Lambda  
- AWS Glue (Spark + PySpark)  

When a new CSV review file is uploaded to S3, Lambda automatically triggers a Glue ETL job that cleans the data, runs analytics with Spark SQL, and writes the results back into another S3 bucket â€” **all without manual action.**

---

# ğŸŒŸ Project Goal

This assignment simulates a real cloud data engineering workflow:

1. Upload product review CSV â†’ S3 landing bucket  
2. Lambda detects file creation event  
3. Lambda starts AWS Glue Spark ETL job  
4. Glue ETL job:  
   - Reads & cleans CSV  
   - Runs **4 Spark SQL queries** (3 additional queries required in instructions)  
   - Saves final results into a processed S3 bucket  

This builds a **no-touch automated pipeline** used in modern data engineering systems.

---

# ğŸ“ Project Repository Structure

```
2025_ITCS6190_Hands-on_L13/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ glue_etl_script.py
â”‚   â””â”€â”€ lambda_function.py
â”‚
â”œâ”€â”€ input/
â”‚   â””â”€â”€ reviews.csv
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ processed-data/
â”‚   â”‚   â””â”€â”€ run-1763497006162-part-r-00000
â”‚   â”‚
â”‚   â””â”€â”€ Athena Results/
â”‚       â”œâ”€â”€ product_rating_avg/
â”‚       â”‚   â””â”€â”€ run-1763496820291-part-r-00000
â”‚       â”œâ”€â”€ daily_review_counts/
â”‚       â”‚   â””â”€â”€ run-1763496825513-part-r-00000
â”‚       â”œâ”€â”€ top_5_customers/
â”‚       â”‚   â””â”€â”€ run-1763496828123-part-r-00000
â”‚       â””â”€â”€ rating_distribution/
â”‚           â””â”€â”€ run-1763496832500-part-r-00000
â”‚
â””â”€â”€ screenshots/
    â”œâ”€â”€ l13_001.jpg
    â”œâ”€â”€ l13_002.jpg
    â”œâ”€â”€ l13_003.jpg
    â”œâ”€â”€ l13_004.jpg
    â”œâ”€â”€ l13_005.jpg
    â”œâ”€â”€ l13_006.jpg
    â”œâ”€â”€ l13_007.jpg
    â”œâ”€â”€ l13_008.jpg
    â””â”€â”€ l13_009.jpg
```

---

# ğŸ—ï¸ Architecture Diagram

```
S3 (Upload)
     â†“
AWS Lambda (Trigger)
     â†“
AWS Glue ETL (Spark Job)
     â†“
S3 (Processed Results)
```

This design is **fully serverless**, automatically scalable, and event-driven.

---

# âš™ï¸ Technologies Used

- **Amazon S3** â€“ raw + processed data storage  
- **AWS Lambda** â€“ event-driven trigger  
- **AWS Glue (Spark)** â€“ ETL + analytics  
- **PySpark & Spark SQL** â€“ data transformations  
- **AWS IAM** â€“ access & execution control  

---

# ğŸš€ Step-by-Step Deployment

## **1ï¸âƒ£ Create S3 Buckets**
Two globally unique buckets:

- Landing bucket  
  `handsonfinallanding-itcs6190-l13-handson-kkim43`
- Processed bucket  
  `handsonfinalprocessed-itcs6190-l13-handson-kkim43`

**Screenshot:**  
![s3 buckets](screenshots/l13_001.jpg)

---

## **2ï¸âƒ£ Upload `reviews.csv` to Landing Bucket**

Uploading this file triggers the entire pipeline.

![upload csv](screenshots/l13_002.jpg)

---

## **3ï¸âƒ£ Create IAM Role for Glue**

Role name: `AWSGlueServiceRole-Reviews`  
Attached policies:

- AWSGlueServiceRole  
- AmazonS3FullAccess (for this assignment)

---

## **4ï¸âƒ£ Create the AWS Glue ETL Job**

- Job Name: **process_reviews_job**
- Script Source: `src/glue_etl_script.py`

![glue script](screenshots/l13_005.jpg)

---

## **5ï¸âƒ£ Create AWS Lambda Trigger Function**

Name: **start_glue_job_trigger**  
Runtime: Python 3.10  
Purpose: Start Glue job on S3 upload

### Add Inline Policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "glue:StartJobRun",
      "Resource": "*"
    }
  ]
}
```

![lambda trigger](screenshots/l13_003.jpg)

---

## **6ï¸âƒ£ Lambda CloudWatch Logs**

Shows job trigger confirmation.

![lambda logs](screenshots/l13_004.jpg)

---

# ğŸ”¥ Glue ETL Processing Details

## âœ” Data Cleaning Steps

- Convert `rating` â†’ integer  
- Fill missing values  
- Convert `review_date` â†’ date  
- Uppercase product_id  
- Default "No review text" for empty text  

---

# ğŸ“Š Spark SQL Analytics (4 Queries)

### **1. Average Rating per Product (Provided)**  
Generates mean rating + review count.

Output folder:
```
output/Athena Results/product_rating_avg/
```

---

### **2. Daily Review Count (Student Query)**  
Shows number of reviews per day.

Output folder:
```
output/Athena Results/daily_review_counts/
```

---

### **3. Top 5 Most Active Customers (Student Query)**  
Identifies users who posted the most reviews.

Output:
```
output/Athena Results/top_5_customers/
```

---

### **4. Rating Distribution (Student Query)**  
Counts reviews for ratings 0â€“5.

Output:
```
output/Athena Results/rating_distribution/
```

---

# ğŸ“ˆ Glue Job Monitoring

Job completed successfully.

![glue monitoring](screenshots/l13_006.jpg)

---

# ğŸ“‚ Output Files (Final Results)

## Processed Dataset
```
output/processed-data/
```

## Analytics Outputs (All 4 Queries)
![processed output](screenshots/l13_007.jpg)
![athena results](screenshots/l13_008.jpg)

---

# ğŸ“„ Sample Output File

![sample file](screenshots/l13_009.jpg)

---

# ğŸ§¹ Cleanup Instructions

To avoid AWS charges:

- Delete S3 buckets  
- Delete Glue ETL job  
- Delete Lambda function  
- Delete IAM role  

---
